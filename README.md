# Top-10-Interview-Question-Machine-Learning-
## 1). What is overfitting?  / Please briefly describe what is bias vs. variance.

Overfitting: The model has tried to capture the sampling error. The model has learned the data’s signal and the noise.


### Bias vs. Variance
- Small Bias: accurate                                            

- Small variance: precise



![](https://github.com/YingchuLo/Top-10-Interview-Question-Machine-Learning-/blob/master/Screen%20Shot%202019-01-11%20at%2010.52.22%20AM.png)



### Bias and variance tradeoff



![](https://github.com/YingchuLo/Top-10-Interview-Question-Machine-Learning-/blob/master/Screen%20Shot%202019-01-11%20at%2010.54.26%20AM.png)



## 2). How do you overcome overfitting? Please list 3-5 practical experience. / What is 'Dimension Curse'? How to prevent?

- Get more data: not always possible/practical

- Subset Selection: keep only a subset of your predictors (i.e, dimensions)

- Regularization: restrict your model’s parameter space

- Dimensionality Reduction: project the data into a lower dimensional space
 


## 3). Please briefly describe the Random Forest classifier. How did it work? Any pros and cons in practical implementation?
## 4). Please describe the difference between GBM tree model and Random Forest.
## 5). What is SVM? what parameters you will need to tune during model training? How is different kernel changing the classification result?
## 6). Briefly rephrase PCA in your own way. How does it work? And tell some goods and bads about it.
## 7). Why doesn't logistic regression use R^2?
## 8). When will you use L1 regularization compared to L2?
## 9). List out at least 4 metrics you will use to evaluate model performance and tell the advantage for each of them. (F1 score, ROC curve, recall, etc…)
## 10). What would you do if you have > 30% missing value in an important field before building the model?
